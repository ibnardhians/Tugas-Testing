{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "c89c6d78-cd49-43b4-99b7-262151427d57",
        "_uuid": "bc244d28a1532f3842dd1739cec5adff4e4b6c79",
        "collapsed": true,
        "trusted": true,
        "id": "G_dYpm1lx50i"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import TimeDistributed, GlobalAveragePooling1D, GlobalAveragePooling2D, BatchNormalization, LSTM, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AveragePooling1D\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dropout, Flatten, Bidirectional, Dense, Activation, TimeDistributed\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from string import ascii_lowercase\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec, Doc2Vec, KeyedVectors\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import itertools, nltk, snowballstemmer, re\n",
        "\n",
        "TaggedDocument = doc2vec.TaggedDocument\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "66eaff70-1bb2-436f-ac31-b85281a2876d",
        "_uuid": "4402fef233db64bf46f51e3bdd0aadc111b11753",
        "collapsed": true,
        "trusted": true,
        "id": "tce-IWEPx50m"
      },
      "cell_type": "code",
      "source": [
        "class LabeledLineSentence(object):\n",
        "    def __init__(self, sources):\n",
        "        self.sources = sources\n",
        "\n",
        "        flipped = {}\n",
        "\n",
        "        # make sure that keys are unique\n",
        "        for key, value in sources.items():\n",
        "            if value not in flipped:\n",
        "                flipped[value] = [key]\n",
        "            else:\n",
        "                raise Exception('Non-unique prefix encountered')\n",
        "\n",
        "    def __iter__(self):\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    yield TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
        "\n",
        "    def to_array(self):\n",
        "        self.sentences = []\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    self.sentences.append(TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
        "        return self.sentences\n",
        "\n",
        "    def sentences_perm(self):\n",
        "        shuffled = list(self.sentences)\n",
        "        random.shuffle(shuffled)\n",
        "        return shuffled"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "eac514b4-5efb-4291-81a3-6730a0cdac61",
        "_uuid": "57bed5b9fd3ad963809205d71d0883a9e38bb9f9",
        "collapsed": true,
        "trusted": true,
        "id": "k9TrkrW7x50n"
      },
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv('deceptive-opinion-spam-corpus.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
        "data = pd.read_csv(\"../input/deceptive-opinion.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5aedff72-437f-4530-9bfe-56165ddd9690",
        "_uuid": "0d7d60b7569b847685579a73ded95641fcfe63d1",
        "collapsed": true,
        "trusted": true,
        "id": "addNa_zGx50n"
      },
      "cell_type": "code",
      "source": [
        "data['polarity'] = np.where(data['polarity']=='positive', 1, 0)\n",
        "data['deceptive'] = np.where(data['deceptive']=='truthful', 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1a30e639-f406-4320-acd4-b190cb0a1f06",
        "_uuid": "fb20c77b00518d320c96687071080ac60b3ae8e7",
        "collapsed": true,
        "trusted": true,
        "id": "ZO66FQ9Ix50o"
      },
      "cell_type": "code",
      "source": [
        "def create_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return [1,1]\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return [1,0]\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return [0,1]\n",
        "    else:\n",
        "        return [0,0]\n",
        "\n",
        "def specific_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return \"TRUE_POSITIVE\"\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return \"FALSE_POSITIVE\"\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return \"TRUE_NEGATIVE\"\n",
        "    else:\n",
        "        return \"FALSE_NEGATIVE\"\n",
        "\n",
        "data['final_class'] = data.apply(create_class, axis=1)\n",
        "data['given_class'] = data.apply(specific_class, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4eaeb14e-a7f3-455e-b43c-64b26bdccd7b",
        "_uuid": "6151435f72f8802262e5b748a4039bbc4074943f",
        "collapsed": true,
        "trusted": true,
        "id": "lxu8uzKSx50p"
      },
      "cell_type": "code",
      "source": [
        "Y = data['final_class']\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "358caced-4efe-454d-b9e6-fb1db61c3cd5",
        "_uuid": "5a2723885623b4e5c62d5cbf04b639450ebcd486",
        "collapsed": true,
        "trusted": true,
        "id": "6Xd32zVTx50q"
      },
      "cell_type": "code",
      "source": [
        "textData = pd.DataFrame(list(data['text'])) # each row is one document; the raw text of the document should be in the 'text_data' column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "727195f6-a01b-478c-9e6e-f63f9e0bcb51",
        "_uuid": "b4b122f69da68f42742cf97da439dc067b1a5a52",
        "collapsed": true,
        "trusted": true,
        "id": "iK6KVWuEx50q"
      },
      "cell_type": "code",
      "source": [
        "# initialize stemmer\n",
        "stemmer = snowballstemmer.EnglishStemmer()\n",
        "\n",
        "# grab stopword list, extend it a bit, and then turn it into a set for later\n",
        "stop = stopwords.words('english')\n",
        "stop.extend(['may','also','zero','one','two','three','four','five','six','seven','eight','nine','ten','across','among','beside','however','yet','within']+list(ascii_lowercase))\n",
        "stoplist = stemmer.stemWords(stop)\n",
        "stoplist = set(stoplist)\n",
        "stop = set(sorted(stop + list(stoplist)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3bf58a8f-b3aa-41d0-af3b-4eca01d7f7bb",
        "_uuid": "8a53f638d29e5bf6c6859dff180ddd45611c6f4a",
        "collapsed": true,
        "trusted": true,
        "id": "6Vg9QubQx50r"
      },
      "cell_type": "code",
      "source": [
        "# remove characters and stoplist words, then generate dictionary of unique words\n",
        "textData[0].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
        "wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*textData[0].str.split(' ')))))).split(\" \"))\n",
        "data['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in textData[0].str.lower().str.split(' ')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0a714384-6691-4178-94f8-6bc7e7771908",
        "_uuid": "7c57c1e8ed3f743e63850290bf5ed6030dcb95e3",
        "collapsed": true,
        "trusted": true,
        "id": "NYnEh8mrx50s"
      },
      "cell_type": "code",
      "source": [
        "# remove all words that don't occur at least 5 times and then stem the resulting docs\n",
        "minimum_count = 1\n",
        "str_frequencies = pd.DataFrame(list(Counter(filter(None,list(itertools.chain(*data['stemmed_text_data'].str.split(' '))))).items()),columns=['word','count'])\n",
        "low_frequency_words = set(str_frequencies[str_frequencies['count'] < minimum_count]['word'])\n",
        "data['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in low_frequency_words, line))) for line in data['stemmed_text_data'].str.split(' ')]\n",
        "data['stemmed_text_data'] = [\" \".join(stemmer.stemWords(re.sub('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ', next_text).split(' '))) for next_text in data['stemmed_text_data']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ad04c9ec-5721-4a55-bcec-afd7f4e150db",
        "_uuid": "3067446c5a55f58f67fcc4d1eb9cb1f0eea8a339",
        "collapsed": true,
        "trusted": true,
        "id": "OEN0CNq1x50t"
      },
      "cell_type": "code",
      "source": [
        "lmtzr = WordNetLemmatizer()\n",
        "w = re.compile(\"\\w+\",re.I)\n",
        "\n",
        "def label_sentences(df, input_point):\n",
        "    labeled_sentences = []\n",
        "    list_sen = []\n",
        "    for index, datapoint in df.iterrows():\n",
        "        tokenized_words = re.findall(w,datapoint[input_point].lower())\n",
        "        labeled_sentences.append(TaggedDocument(words=tokenized_words, tags=['SENT_%s' %index]))\n",
        "        list_sen.append(tokenized_words)\n",
        "    return labeled_sentences, list_sen\n",
        "\n",
        "def train_doc2vec_model(labeled_sentences):\n",
        "    model = Doc2Vec(min_count=1, window=9, size=512, sample=1e-4, negative=5, workers=7)\n",
        "    model.build_vocab(labeled_sentences)\n",
        "    pretrained_weights = model.wv.syn0\n",
        "    vocab_size, embedding_size = pretrained_weights.shape\n",
        "    model.train(labeled_sentences, total_examples=vocab_size, epochs=400)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1cab13d3-d608-4e49-89fc-5076835f4ab2",
        "_uuid": "2bf3e3c0bef170d0601e098b159ea758a3aeb461",
        "collapsed": true,
        "trusted": true,
        "id": "OZXJyeDwx50u"
      },
      "cell_type": "code",
      "source": [
        "textData = data['stemmed_text_data'].to_frame().reset_index()\n",
        "sen, corpus = label_sentences(textData, 'stemmed_text_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a84fe753-6f2c-44d9-990a-af41a34a282b",
        "_uuid": "fdf41cbef8aed8c77625debd753d88b37ed108ca",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "id": "8iHaupE-x50u"
      },
      "cell_type": "code",
      "source": [
        "doc2vec_model = train_doc2vec_model(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bcbdbe26-8a3f-4350-8633-3d9aaf0c172e",
        "_uuid": "4de396cec1aeb9a63648d1aa6ac93d3914c05b78",
        "collapsed": true,
        "trusted": true,
        "id": "cgb17uP7x50u"
      },
      "cell_type": "code",
      "source": [
        "doc2vec_model.save(\"doc2vec_model_opinion_corpus.d2v\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3fe3f2b0-b681-421c-9adc-682796f97387",
        "_uuid": "161eecd1a6baac4e39bfa9f5b7620ece1165ecc7",
        "collapsed": true,
        "trusted": true,
        "id": "TMKOaeP1x50v"
      },
      "cell_type": "code",
      "source": [
        "doc2vec_model = Doc2Vec.load(\"doc2vec_model_opinion_corpus.d2v\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e4bc5f09-3bbf-4ddf-97e1-e497c210d6ae",
        "_uuid": "fb98f66f270a63bfd0d82bb1cfcacd18dae40873",
        "collapsed": true,
        "trusted": true,
        "id": "ijCDsZR7x50v"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "tfidf1 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,1))\n",
        "result_train1 = tfidf1.fit_transform(corpus)\n",
        "\n",
        "tfidf2 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,2))\n",
        "result_train2 = tfidf2.fit_transform(corpus)\n",
        "\n",
        "tfidf3 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,3))\n",
        "result_train3 = tfidf3.fit_transform(corpus)\n",
        "\n",
        "svd = TruncatedSVD(n_components=512, n_iter=40, random_state=34)\n",
        "tfidf_data1 = svd.fit_transform(result_train1)\n",
        "tfidf_data2 = svd.fit_transform(result_train2)\n",
        "tfidf_data3 = svd.fit_transform(result_train3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ef1dbd5a-e69e-4b56-9ff3-c74ce62fdfdd",
        "_uuid": "bfd26d39cd104b2bf443fc52992437751fa852b7",
        "collapsed": true,
        "trusted": true,
        "id": "Y6qw5kNxx50w"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "temp_textData = pd.DataFrame(list(data['text']))\n",
        "\n",
        "overall_pos_tags_tokens = []\n",
        "overall_pos = []\n",
        "overall_tokens = []\n",
        "overall_dep = []\n",
        "\n",
        "for i in range(1600):\n",
        "    doc = nlp(temp_textData[0][i])\n",
        "    given_pos_tags_tokens = []\n",
        "    given_pos = []\n",
        "    given_tokens = []\n",
        "    given_dep = []\n",
        "    for token in doc:\n",
        "        output = \"%s_%s\" % (token.pos_, token.tag_)\n",
        "        given_pos_tags_tokens.append(output)\n",
        "        given_pos.append(token.pos_)\n",
        "        given_tokens.append(token.tag_)\n",
        "        given_dep.append(token.dep_)\n",
        "\n",
        "    overall_pos_tags_tokens.append(given_pos_tags_tokens)\n",
        "    overall_pos.append(given_pos)\n",
        "    overall_tokens.append(given_tokens)\n",
        "    overall_dep.append(given_dep)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dbcdbe6d-0c7f-44dc-a9d6-0f240d339885",
        "_uuid": "1c19ee8bd8fb5862cc5d25225b0684dd755ee8b2",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "id": "raX9bUEsx50w"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "count = CountVectorizer(tokenizer=lambda i:i, lowercase=False)\n",
        "pos_tags_data = count.fit_transform(overall_pos_tags_tokens).todense()\n",
        "pos_data = count.fit_transform(overall_pos).todense()\n",
        "tokens_data = count.fit_transform(overall_tokens).todense()\n",
        "dep_data = count.fit_transform(overall_dep).todense()\n",
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_pos_tags_data = min_max_scaler.fit_transform(pos_tags_data)\n",
        "normalized_pos_data = min_max_scaler.fit_transform(pos_data)\n",
        "normalized_tokens_data = min_max_scaler.fit_transform(tokens_data)\n",
        "normalized_dep_data = min_max_scaler.fit_transform(dep_data)\n",
        "\n",
        "final_pos_tags_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_tokens_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_dep_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_tags_data[:normalized_pos_tags_data.shape[0],:normalized_pos_tags_data.shape[1]] = normalized_pos_tags_data\n",
        "final_pos_data[:normalized_pos_data.shape[0],:normalized_pos_data.shape[1]] = normalized_pos_data\n",
        "final_tokens_data[:normalized_tokens_data.shape[0],:normalized_tokens_data.shape[1]] = normalized_tokens_data\n",
        "final_dep_data[:normalized_dep_data.shape[0],:normalized_dep_data.shape[1]] = normalized_dep_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "57e0d758-a2b5-4a24-a60e-cfa2df06572f",
        "_uuid": "e6474b59e587fbafbd2ea8801a50ef6d787a5868",
        "collapsed": true,
        "trusted": true,
        "id": "YqUp2Cwox50x"
      },
      "cell_type": "code",
      "source": [
        "maxlength = []\n",
        "for i in range(0,len(sen)):\n",
        "    maxlength.append(len(sen[i][0]))\n",
        "\n",
        "print(max(maxlength))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dda366e0-43b7-4b35-8f31-c41d572375d4",
        "_uuid": "f53332f2ce02fee41617d0205c524ebe26c4a90d",
        "collapsed": true,
        "trusted": true,
        "id": "Zpefd157x50x"
      },
      "cell_type": "code",
      "source": [
        "def vectorize_comments(df,d2v_model):\n",
        "    y = []\n",
        "    comments = []\n",
        "    for i in range(0,df.shape[0]):\n",
        "        label = 'SENT_%s' %i\n",
        "        comments.append(d2v_model.docvecs[label])\n",
        "    df['vectorized_comments'] = comments\n",
        "\n",
        "    return df\n",
        "\n",
        "textData = vectorize_comments(textData,doc2vec_model)\n",
        "print (textData.head(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3fafb37f-8eee-4de3-80a0-0587f7075bc7",
        "_uuid": "b978f4c41b6adc60490f0c6e048b3195249b9629",
        "collapsed": true,
        "trusted": true,
        "id": "KO3JP2RJx50x"
      },
      "cell_type": "code",
      "source": [
        "# load the whole embedding into memory\n",
        "#embeddings_index = dict()\n",
        "#f = open('glove/glove.6B.300d.txt')\n",
        "#for line in f:\n",
        "#    values = line.split()\n",
        "#    word = values[0]\n",
        "#    coefs = np.asarray(values[1:], dtype='float32')\n",
        "#    embeddings_index[word] = coefs\n",
        "#f.close()\n",
        "#print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "08eadfe1-897d-4e46-bc46-95a5e452a618",
        "_uuid": "b99088a77d8d46e52ba16259ffba4b655265f3cd",
        "collapsed": true,
        "trusted": true,
        "id": "w2YvjC1gx50x"
      },
      "cell_type": "code",
      "source": [
        "#from nltk.corpus import stopwords\n",
        "\n",
        "#glove_data = np.zeros(shape=(1600, 800, 512)).astype(np.float32)\n",
        "#temp_textData = data['text'].to_frame().reset_index()\n",
        "#sen2, corpus2 = label_sentences(temp_textData, 'text')\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "#test_word = np.zeros(512).astype(np.float32)\n",
        "#final_matrix = np.zeros(512).astype(np.float32)\n",
        "#final_sizes = []\n",
        "\n",
        "#count = True\n",
        "\n",
        "#for i in range(1600):\n",
        "#    for j in sen2[i][0]:\n",
        "#        if j in embeddings_index and j not in stop_words:\n",
        "#            test_word[:300] = embeddings_index[j]\n",
        "#            if count == True:\n",
        "#                final_matrix = test_word\n",
        "#                count = False\n",
        "#            else:\n",
        "#                final_matrix = np.vstack((final_matrix, test_word))\n",
        "\n",
        "#    final_sizes.append(final_matrix.shape[0])\n",
        "#    final_matrix = np.zeros(512).astype(np.float32)\n",
        "#    glove_data[i,:final_matrix.shape[0],:] = final_matrix\n",
        "#    count = True\n",
        "\n",
        "#print(max(final_sizes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b2a454a7-34dd-4eb5-93ac-ac891eb7e9fe",
        "_uuid": "4ebed90a6e28ab4de06c3037761106b6f6a0f4e5",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "id": "L9eTrYBgx50y"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import cross_validation\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "X_train, X_test, y_train, y_test = cross_validation.train_test_split(textData[\"vectorized_comments\"].T.tolist(),\n",
        "                                                                     dummy_y,\n",
        "                                                                     test_size=0.1,\n",
        "                                                                     random_state=56)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "19c5322f-c579-4b9b-91fb-ebca06c6715f",
        "_uuid": "2c4e424e1c6f1a3a6abb7beb8eec6f33d3bc2622",
        "collapsed": true,
        "trusted": true,
        "id": "skmsroWsx50y"
      },
      "cell_type": "code",
      "source": [
        "X = np.array(textData[\"vectorized_comments\"].T.tolist()).reshape((1,1600,512))\n",
        "y = np.array(dummy_y).reshape((1600,4))\n",
        "X_train2 = np.array(X_train).reshape((1,1440,512))\n",
        "y_train2 = np.array(y_train).reshape((1,1440,4))\n",
        "X_test2 = np.array(X_test).reshape((1,160,512))\n",
        "y_test2 = np.array(y_test).reshape((1,160,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "00a664f3-390b-4f40-9580-8ca6c3021678",
        "_uuid": "87b1e9251955272eee9475662179a60dc030a399",
        "collapsed": true,
        "trusted": true,
        "id": "SiPUaUDex50z"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "Xtemp = textData[\"vectorized_comments\"].T.tolist()\n",
        "ytemp = data['given_class']\n",
        "training_indices = []\n",
        "testing_indices = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "skf.get_n_splits(Xtemp, ytemp)\n",
        "\n",
        "for train_index, test_index in skf.split(Xtemp, ytemp):\n",
        "    training_indices.append(train_index)\n",
        "    testing_indices.append(test_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0fac1165-69ad-4576-9729-8857d625cef1",
        "_uuid": "45bc95366bbb2e542be56aaf5f73582fd65062d4",
        "collapsed": true,
        "trusted": true,
        "id": "E0cDH7ECx500"
      },
      "cell_type": "code",
      "source": [
        "def extractTrainingAndTestingData(givenIndex):\n",
        "    X_train3 = np.zeros(shape=(1440, max(maxlength)+10, 512)).astype(np.float32)\n",
        "    Y_train3 = np.zeros(shape=(1440, 4)).astype(np.float32)\n",
        "    X_test3 = np.zeros(shape=(160, max(maxlength)+10, 512)).astype(np.float32)\n",
        "    Y_test3 = np.zeros(shape=(160, 4)).astype(np.float32)\n",
        "\n",
        "    empty_word = np.zeros(512).astype(np.float32)\n",
        "\n",
        "    count_i = 0\n",
        "    for i in training_indices[givenIndex]:\n",
        "        len1 = len(sen[i][0])\n",
        "        average_vector1 = np.zeros(512).astype(np.float32)\n",
        "        average_vector2 = np.zeros(512).astype(np.float32)\n",
        "        average_vector3 = np.zeros(512).astype(np.float32)\n",
        "        for j in range(max(maxlength)+10):\n",
        "            if j < len1:\n",
        "                X_train3[count_i,j,:] = doc2vec_model[sen[i][0][j]]\n",
        "                average_vector1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            #elif j >= len1 and j < len1 + 379:\n",
        "            #    X_train3[count_i,j,:] = glove_data[i, j-len1, :]\n",
        "            elif j == len1:\n",
        "                X_train3[count_i,j,:] = tfidf_data1[i]\n",
        "            elif j == len1 + 1:\n",
        "                X_train3[count_i,j,:] = tfidf_data2[i]\n",
        "            elif j == len1+2:\n",
        "                X_train3[count_i,j,:] = tfidf_data3[i]\n",
        "            elif j == len1+3:\n",
        "                X_train3[count_i,j,:] = average_vector1\n",
        "            elif j == len1+4:\n",
        "                X_train3[count_i,j,:] = average_vector2\n",
        "            elif j == len1+5:\n",
        "                X_train3[count_i,j,:] = average_vector3\n",
        "            elif j == len1+6:\n",
        "                X_train3[count_i,j,:] = final_pos_tags_data[i]\n",
        "            elif j == len1+7:\n",
        "                X_train3[count_i,j,:] = final_pos_data[i]\n",
        "            elif j == len1+8:\n",
        "                X_train3[count_i,j,:] = final_tokens_data[i]\n",
        "            elif j == len1+9:\n",
        "                X_train3[count_i,j,:] = final_dep_data[i]\n",
        "            else:\n",
        "                X_train3[count_i,j,:] = empty_word\n",
        "\n",
        "        Y_train3[count_i,:] = dummy_y[i]\n",
        "        count_i += 1\n",
        "\n",
        "\n",
        "    count_i = 0\n",
        "    for i in testing_indices[givenIndex]:\n",
        "        len1 = len(sen[i][0])\n",
        "        average_vector1 = np.zeros(512).astype(np.float32)\n",
        "        average_vector2 = np.zeros(512).astype(np.float32)\n",
        "        average_vector3 = np.zeros(512).astype(np.float32)\n",
        "        for j in range(max(maxlength)+10):\n",
        "            if j < len1:\n",
        "                X_test3[count_i,j,:] = doc2vec_model[sen[i][0][j]]\n",
        "                average_vector1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            #elif j >= len1 and j < len1 + 379:\n",
        "            #    X_test3[count_i,j,:] = glove_data[i, j-len1, :]\n",
        "            elif j == len1:\n",
        "                X_test3[count_i,j,:] = tfidf_data1[i]\n",
        "            elif j == len1 + 1:\n",
        "                X_test3[count_i,j,:] = tfidf_data2[i]\n",
        "            elif j == len1+2:\n",
        "                X_test3[count_i,j,:] = tfidf_data3[i]\n",
        "            elif j == len1+3:\n",
        "                X_test3[count_i,j,:] = average_vector1\n",
        "            elif j == len1+4:\n",
        "                X_test3[count_i,j,:] = average_vector2\n",
        "            elif j == len1+5:\n",
        "                X_test3[count_i,j,:] = average_vector3\n",
        "            elif j == len1+6:\n",
        "                X_test3[count_i,j,:] = final_pos_tags_data[i]\n",
        "            elif j == len1+7:\n",
        "                X_test3[count_i,j,:] = final_pos_data[i]\n",
        "            elif j == len1+8:\n",
        "                X_test3[count_i,j,:] = final_tokens_data[i]\n",
        "            elif j == len1+9:\n",
        "                X_test3[count_i,j,:] = final_dep_data[i]\n",
        "            else:\n",
        "                X_test3[count_i,j,:] = empty_word\n",
        "\n",
        "        Y_test3[count_i,:] = dummy_y[i]\n",
        "        count_i += 1\n",
        "\n",
        "    return X_train3, X_test3, Y_train3, Y_test3\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1c0d8684-466d-48e6-9afa-bc30ff388a26",
        "_uuid": "b7d99866c070dbf1851fa20d0a242cf1623997ee",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "id": "L-Mvh9TSx500"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=128, kernel_size=9, padding='same', activation='relu', input_shape=(max(maxlength)+10,512)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters=128, kernel_size=7, padding='same', activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "#model.add(Bidirectional(LSTM(50, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(50, dropout=0.25, recurrent_dropout=0.2)))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a8ec219d-90d2-4f03-97ac-52323bd112c8",
        "_uuid": "a81ad9005cf0c09a134930e20e9e14d3809e3c82",
        "collapsed": true,
        "scrolled": false,
        "trusted": true,
        "id": "V8B2yOi2x501"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "final_accuracies = []\n",
        "\n",
        "filename = 'weights.best.from_scratch%s.hdf5' % 9\n",
        "checkpointer = ModelCheckpoint(filepath=filename, verbose=1, save_best_only=True)\n",
        "X_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(9)\n",
        "model.fit(X_train3, Y_train3, epochs=10, batch_size=512, callbacks=[checkpointer], validation_data=(X_test3, Y_test3))\n",
        "model.load_weights(filename)\n",
        "\n",
        "for i in range(10):\n",
        "    filename = 'weights.best.from_scratch%s.hdf5' % i\n",
        "    checkpointer = ModelCheckpoint(filepath=filename, verbose=1, save_best_only=True)\n",
        "    X_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(i)\n",
        "    model.fit(X_train3, Y_train3, epochs=10, batch_size=512, callbacks=[checkpointer], validation_data=(X_test3, Y_test3))\n",
        "    model.load_weights(filename)\n",
        "    predicted = np.rint(model.predict(X_test3))\n",
        "    final_accuracies.append(accuracy_score(Y_test3, predicted))\n",
        "    print(accuracy_score(Y_test3, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d7883c70-f74b-4389-85fe-ff575dd15a61",
        "_uuid": "2d96181354f0499ae115b2041997108afa8946dd",
        "collapsed": true,
        "trusted": true,
        "id": "qRtLyU1Rx502"
      },
      "cell_type": "code",
      "source": [
        "print(sum(final_accuracies) / len(final_accuracies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a81ce0af-44a8-4081-8a54-73c8ffc97bdf",
        "_uuid": "e9c5ad94f4a4c5ad58e9e588782fc544839b8169",
        "collapsed": true,
        "trusted": true,
        "id": "t7CMc2yux503"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}