{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "c89c6d78-cd49-43b4-99b7-262151427d57",
        "_uuid": "bc244d28a1532f3842dd1739cec5adff4e4b6c79",
        "collapsed": true,
        "trusted": true,
        "id": "G_dYpm1lx50i"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import TimeDistributed, GlobalAveragePooling1D, GlobalAveragePooling2D, BatchNormalization, LSTM, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AveragePooling1D\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dropout, Flatten, Bidirectional, Dense, Activation, TimeDistributed\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from string import ascii_lowercase\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec, Doc2Vec, KeyedVectors\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import itertools, nltk, snowballstemmer, re\n",
        "\n",
        "TaggedDocument = doc2vec.TaggedDocument\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "66eaff70-1bb2-436f-ac31-b85281a2876d",
        "_uuid": "4402fef233db64bf46f51e3bdd0aadc111b11753",
        "collapsed": true,
        "trusted": true,
        "id": "tce-IWEPx50m"
      },
      "cell_type": "code",
      "source": [
        "class LabeledLineSentence(object):\n",
        "    def __init__(self, sources):\n",
        "        self.sources = sources\n",
        "\n",
        "        flipped = {}\n",
        "\n",
        "        # make sure that keys are unique\n",
        "        for key, value in sources.items():\n",
        "            if value not in flipped:\n",
        "                flipped[value] = [key]\n",
        "            else:\n",
        "                raise Exception('Non-unique prefix encountered')\n",
        "\n",
        "    def __iter__(self):\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    yield TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
        "\n",
        "    def to_array(self):\n",
        "        self.sentences = []\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    self.sentences.append(TaggedDocument(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
        "        return self.sentences\n",
        "\n",
        "    def sentences_perm(self):\n",
        "        shuffled = list(self.sentences)\n",
        "        random.shuffle(shuffled)\n",
        "        return shuffled"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "eac514b4-5efb-4291-81a3-6730a0cdac61",
        "_uuid": "57bed5b9fd3ad963809205d71d0883a9e38bb9f9",
        "collapsed": true,
        "trusted": true,
        "id": "k9TrkrW7x50n"
      },
      "cell_type": "code",
      "source": [
        "#data = pd.read_csv('deceptive-opinion-spam-corpus.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
        "data = pd.read_csv(\"./input/deceptive-opinion.csv\")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5aedff72-437f-4530-9bfe-56165ddd9690",
        "_uuid": "0d7d60b7569b847685579a73ded95641fcfe63d1",
        "collapsed": true,
        "trusted": true,
        "id": "addNa_zGx50n"
      },
      "cell_type": "code",
      "source": [
        "data['polarity'] = np.where(data['polarity']=='positive', 1, 0)\n",
        "data['deceptive'] = np.where(data['deceptive']=='truthful', 1, 0)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1a30e639-f406-4320-acd4-b190cb0a1f06",
        "_uuid": "fb20c77b00518d320c96687071080ac60b3ae8e7",
        "collapsed": true,
        "trusted": true,
        "id": "ZO66FQ9Ix50o"
      },
      "cell_type": "code",
      "source": [
        "def create_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return [1,1]\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return [1,0]\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return [0,1]\n",
        "    else:\n",
        "        return [0,0]\n",
        "\n",
        "def specific_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return \"TRUE_POSITIVE\"\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return \"FALSE_POSITIVE\"\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return \"TRUE_NEGATIVE\"\n",
        "    else:\n",
        "        return \"FALSE_NEGATIVE\"\n",
        "\n",
        "data['final_class'] = data.apply(create_class, axis=1)\n",
        "data['given_class'] = data.apply(specific_class, axis=1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4eaeb14e-a7f3-455e-b43c-64b26bdccd7b",
        "_uuid": "6151435f72f8802262e5b748a4039bbc4074943f",
        "collapsed": true,
        "trusted": true,
        "id": "lxu8uzKSx50p"
      },
      "cell_type": "code",
      "source": [
        "# Assuming 'data' is a DataFrame and 'final_class' is the column with class labels\n",
        "Y = data['final_class']\n",
        "\n",
        "# Flatten the list if necessary (depends on the structure of 'Y')\n",
        "# If 'Y' is already a flat list, you can skip this step\n",
        "Y = [item for sublist in Y for item in sublist] if isinstance(Y[0], list) else Y\n",
        "\n",
        "# Encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# Convert integers to dummy variables (i.e., one hot encoded)\n",
        "dummy_y = to_categorical(encoded_Y)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "358caced-4efe-454d-b9e6-fb1db61c3cd5",
        "_uuid": "5a2723885623b4e5c62d5cbf04b639450ebcd486",
        "collapsed": true,
        "trusted": true,
        "id": "6Xd32zVTx50q"
      },
      "cell_type": "code",
      "source": [
        "textData = pd.DataFrame(list(data['text'])) # each row is one document; the raw text of the document should be in the 'text_data' column"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "727195f6-a01b-478c-9e6e-f63f9e0bcb51",
        "_uuid": "b4b122f69da68f42742cf97da439dc067b1a5a52",
        "collapsed": true,
        "trusted": true,
        "id": "iK6KVWuEx50q"
      },
      "cell_type": "code",
      "source": [
        "# initialize stemmer\n",
        "stemmer = snowballstemmer.EnglishStemmer()\n",
        "\n",
        "# grab stopword list, extend it a bit, and then turn it into a set for later\n",
        "stop = stopwords.words('english')\n",
        "stop.extend(['may','also','zero','one','two','three','four','five','six','seven','eight','nine','ten','across','among','beside','however','yet','within']+list(ascii_lowercase))\n",
        "stoplist = stemmer.stemWords(stop)\n",
        "stoplist = set(stoplist)\n",
        "stop = set(sorted(stop + list(stoplist)))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3bf58a8f-b3aa-41d0-af3b-4eca01d7f7bb",
        "_uuid": "8a53f638d29e5bf6c6859dff180ddd45611c6f4a",
        "collapsed": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vg9QubQx50r",
        "outputId": "7f86853b-7a71-42d0-a8db-6d095b253d82"
      },
      "cell_type": "code",
      "source": [
        "# remove characters and stoplist words, then generate dictionary of unique words\n",
        "textData[0].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
        "wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*textData[0].str.split(' ')))))).split(\" \"))\n",
        "data['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in textData[0].str.lower().str.split(' ')]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-02d393780e60>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  textData[0].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "0a714384-6691-4178-94f8-6bc7e7771908",
        "_uuid": "7c57c1e8ed3f743e63850290bf5ed6030dcb95e3",
        "collapsed": true,
        "trusted": true,
        "id": "NYnEh8mrx50s"
      },
      "cell_type": "code",
      "source": [
        "# remove all words that don't occur at least 5 times and then stem the resulting docs\n",
        "minimum_count = 1\n",
        "str_frequencies = pd.DataFrame(list(Counter(filter(None,list(itertools.chain(*data['stemmed_text_data'].str.split(' '))))).items()),columns=['word','count'])\n",
        "low_frequency_words = set(str_frequencies[str_frequencies['count'] < minimum_count]['word'])\n",
        "data['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in low_frequency_words, line))) for line in data['stemmed_text_data'].str.split(' ')]\n",
        "data['stemmed_text_data'] = [\" \".join(stemmer.stemWords(re.sub('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ', next_text).split(' '))) for next_text in data['stemmed_text_data']]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ad04c9ec-5721-4a55-bcec-afd7f4e150db",
        "_uuid": "3067446c5a55f58f67fcc4d1eb9cb1f0eea8a339",
        "collapsed": true,
        "trusted": true,
        "id": "OEN0CNq1x50t"
      },
      "cell_type": "code",
      "source": [
        "lmtzr = WordNetLemmatizer()\n",
        "w = re.compile(\"\\w+\",re.I)\n",
        "\n",
        "def label_sentences(df, input_point):\n",
        "    labeled_sentences = []\n",
        "    list_sen = []\n",
        "    for index, datapoint in df.iterrows():\n",
        "        tokenized_words = re.findall(w,datapoint[input_point].lower())\n",
        "        labeled_sentences.append(TaggedDocument(words=tokenized_words, tags=['SENT_%s' %index]))\n",
        "        list_sen.append(tokenized_words)\n",
        "    return labeled_sentences, list_sen\n",
        "\n",
        "def train_doc2vec_model(labeled_sentences):\n",
        "    model = Doc2Vec(vector_size=512, window=9, min_count=1, sample=1e-4, negative=5, workers=7, epochs=400)\n",
        "    model.build_vocab(labeled_sentences)\n",
        "    model.train(labeled_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1cab13d3-d608-4e49-89fc-5076835f4ab2",
        "_uuid": "2bf3e3c0bef170d0601e098b159ea758a3aeb461",
        "collapsed": true,
        "trusted": true,
        "id": "OZXJyeDwx50u"
      },
      "cell_type": "code",
      "source": [
        "textData = data['stemmed_text_data'].to_frame().reset_index()\n",
        "sen, corpus = label_sentences(textData, 'stemmed_text_data')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a84fe753-6f2c-44d9-990a-af41a34a282b",
        "_uuid": "fdf41cbef8aed8c77625debd753d88b37ed108ca",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "id": "8iHaupE-x50u"
      },
      "cell_type": "code",
      "source": [
        "doc2vec_model = train_doc2vec_model(sen)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bcbdbe26-8a3f-4350-8633-3d9aaf0c172e",
        "_uuid": "4de396cec1aeb9a63648d1aa6ac93d3914c05b78",
        "collapsed": true,
        "trusted": true,
        "id": "cgb17uP7x50u"
      },
      "cell_type": "code",
      "source": [
        "doc2vec_model.save(\"doc2vec_model_opinion_corpus.d2v\")"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3fe3f2b0-b681-421c-9adc-682796f97387",
        "_uuid": "161eecd1a6baac4e39bfa9f5b7620ece1165ecc7",
        "collapsed": true,
        "trusted": true,
        "id": "TMKOaeP1x50v"
      },
      "cell_type": "code",
      "source": [
        "doc2vec_model = Doc2Vec.load(\"doc2vec_model_opinion_corpus.d2v\")"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e4bc5f09-3bbf-4ddf-97e1-e497c210d6ae",
        "_uuid": "fb98f66f270a63bfd0d82bb1cfcacd18dae40873",
        "collapsed": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijCDsZR7x50v",
        "outputId": "d67042e0-3534-405d-ccf2-f0dfdfbaf2e4"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "tfidf1 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,1))\n",
        "result_train1 = tfidf1.fit_transform(corpus)\n",
        "\n",
        "tfidf2 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,2))\n",
        "result_train2 = tfidf2.fit_transform(corpus)\n",
        "\n",
        "tfidf3 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,3))\n",
        "result_train3 = tfidf3.fit_transform(corpus)\n",
        "\n",
        "svd = TruncatedSVD(n_components=512, n_iter=40, random_state=34)\n",
        "tfidf_data1 = svd.fit_transform(result_train1)\n",
        "tfidf_data2 = svd.fit_transform(result_train2)\n",
        "tfidf_data3 = svd.fit_transform(result_train3)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "ef1dbd5a-e69e-4b56-9ff3-c74ce62fdfdd",
        "_uuid": "bfd26d39cd104b2bf443fc52992437751fa852b7",
        "collapsed": true,
        "trusted": true,
        "id": "Y6qw5kNxx50w"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "temp_textData = pd.DataFrame(list(data['text']))\n",
        "\n",
        "overall_pos_tags_tokens = []\n",
        "overall_pos = []\n",
        "overall_tokens = []\n",
        "overall_dep = []\n",
        "\n",
        "for i in range(1600):\n",
        "    doc = nlp(temp_textData[0][i])\n",
        "    given_pos_tags_tokens = []\n",
        "    given_pos = []\n",
        "    given_tokens = []\n",
        "    given_dep = []\n",
        "    for token in doc:\n",
        "        output = \"%s_%s\" % (token.pos_, token.tag_)\n",
        "        given_pos_tags_tokens.append(output)\n",
        "        given_pos.append(token.pos_)\n",
        "        given_tokens.append(token.tag_)\n",
        "        given_dep.append(token.dep_)\n",
        "\n",
        "    overall_pos_tags_tokens.append(given_pos_tags_tokens)\n",
        "    overall_pos.append(given_pos)\n",
        "    overall_tokens.append(given_tokens)\n",
        "    overall_dep.append(given_dep)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "dbcdbe6d-0c7f-44dc-a9d6-0f240d339885",
        "_uuid": "1c19ee8bd8fb5862cc5d25225b0684dd755ee8b2",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raX9bUEsx50w",
        "outputId": "f041ce17-5eaa-4d8b-ad55-095fc8b26e4f"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "count = CountVectorizer(tokenizer=lambda i: i, lowercase=False)\n",
        "pos_tags_data = count.fit_transform(overall_pos_tags_tokens).todense()\n",
        "pos_data = count.fit_transform(overall_pos).todense()\n",
        "tokens_data = count.fit_transform(overall_tokens).todense()\n",
        "dep_data = count.fit_transform(overall_dep).todense()\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "pos_tags_data = np.asarray(pos_tags_data)\n",
        "pos_data = np.asarray(pos_data)\n",
        "tokens_data = np.asarray(tokens_data)\n",
        "dep_data = np.asarray(dep_data)\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_pos_tags_data = min_max_scaler.fit_transform(pos_tags_data)\n",
        "normalized_pos_data = min_max_scaler.fit_transform(pos_data)\n",
        "normalized_tokens_data = min_max_scaler.fit_transform(tokens_data)\n",
        "normalized_dep_data = min_max_scaler.fit_transform(dep_data)\n",
        "\n",
        "final_pos_tags_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_tokens_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_dep_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_tags_data[:normalized_pos_tags_data.shape[0], :normalized_pos_tags_data.shape[1]] = normalized_pos_tags_data\n",
        "final_pos_data[:normalized_pos_data.shape[0], :normalized_pos_data.shape[1]] = normalized_pos_data\n",
        "final_tokens_data[:normalized_tokens_data.shape[0], :normalized_tokens_data.shape[1]] = normalized_tokens_data\n",
        "final_dep_data[:normalized_dep_data.shape[0], :normalized_dep_data.shape[1]] = normalized_dep_data"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "57e0d758-a2b5-4a24-a60e-cfa2df06572f",
        "_uuid": "e6474b59e587fbafbd2ea8801a50ef6d787a5868",
        "collapsed": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqUp2Cwox50x",
        "outputId": "c3e7d64a-3839-4f96-e28b-8f3b62a5fabf"
      },
      "cell_type": "code",
      "source": [
        "maxlength = []\n",
        "for i in range(0,len(sen)):\n",
        "    maxlength.append(len(sen[i][0]))\n",
        "\n",
        "print(max(maxlength))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "370\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "dda366e0-43b7-4b35-8f31-c41d572375d4",
        "_uuid": "f53332f2ce02fee41617d0205c524ebe26c4a90d",
        "collapsed": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpefd157x50x",
        "outputId": "f977153b-165c-4439-c761-e2cc39466620"
      },
      "cell_type": "code",
      "source": [
        "def vectorize_comments(df,d2v_model):\n",
        "    y = []\n",
        "    comments = []\n",
        "    for i in range(0,df.shape[0]):\n",
        "        label = 'SENT_%s' %i\n",
        "        comments.append(d2v_model.docvecs[label])\n",
        "    df['vectorized_comments'] = comments\n",
        "\n",
        "    return df\n",
        "\n",
        "textData = vectorize_comments(textData,doc2vec_model)\n",
        "print (textData.head(2))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index                                  stemmed_text_data  \\\n",
            "0      0  stay night getaway famili thursday tripl aaa r...   \n",
            "1      1  tripl rate upgrad view room less $ includ brea...   \n",
            "\n",
            "                                 vectorized_comments  \n",
            "0  [0.26989526, -1.6786005, 0.47566018, 1.0536623...  \n",
            "1  [-0.29568905, 0.2535211, -0.15074827, 0.472897...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-e2c23a26370d>:6: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  comments.append(d2v_model.docvecs[label])\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "3fafb37f-8eee-4de3-80a0-0587f7075bc7",
        "_uuid": "b978f4c41b6adc60490f0c6e048b3195249b9629",
        "collapsed": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KO3JP2RJx50x",
        "outputId": "dc257866-d73b-4fba-f5c4-cced0a9f3fb6"
      },
      "cell_type": "code",
      "source": [
        "# load the whole embedding into memory\n",
        "# embeddings_index = dict()\n",
        "# f = open('glove/glove.6B.300d.txt')\n",
        "# for line in f:\n",
        "#    values = line.split()\n",
        "#    word = values[0]\n",
        "#    coefs = np.asarray(values[1:], dtype='float32')\n",
        "#    embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "# print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'glove/glove.6B.300d.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-22fdd85c40a5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the whole embedding into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove/glove.6B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove/glove.6B.300d.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "08eadfe1-897d-4e46-bc46-95a5e452a618",
        "_uuid": "b99088a77d8d46e52ba16259ffba4b655265f3cd",
        "collapsed": true,
        "trusted": true,
        "id": "w2YvjC1gx50x"
      },
      "cell_type": "code",
      "source": [
        "#from nltk.corpus import stopwords\n",
        "\n",
        "#glove_data = np.zeros(shape=(1600, 800, 512)).astype(np.float32)\n",
        "#temp_textData = data['text'].to_frame().reset_index()\n",
        "#sen2, corpus2 = label_sentences(temp_textData, 'text')\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "#test_word = np.zeros(512).astype(np.float32)\n",
        "#final_matrix = np.zeros(512).astype(np.float32)\n",
        "#final_sizes = []\n",
        "\n",
        "#count = True\n",
        "\n",
        "#for i in range(1600):\n",
        "#    for j in sen2[i][0]:\n",
        "#        if j in embeddings_index and j not in stop_words:\n",
        "#            test_word[:300] = embeddings_index[j]\n",
        "#            if count == True:\n",
        "#                final_matrix = test_word\n",
        "#                count = False\n",
        "#            else:\n",
        "#                final_matrix = np.vstack((final_matrix, test_word))\n",
        "\n",
        "#    final_sizes.append(final_matrix.shape[0])\n",
        "#    final_matrix = np.zeros(512).astype(np.float32)\n",
        "#    glove_data[i,:final_matrix.shape[0],:] = final_matrix\n",
        "#    count = True\n",
        "\n",
        "#print(max(final_sizes))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Check the shapes of the input variables\n",
        "print(f\"Shape of textData['vectorized_comments']: {textData['vectorized_comments'].shape}\")\n",
        "print(f\"Shape of dummy_y: {dummy_y.shape}\")\n",
        "\n",
        "# Ensure the data alignment\n",
        "min_length = len(textData[\"vectorized_comments\"])\n",
        "dummy_y = dummy_y[:min_length]\n",
        "\n",
        "# Verify the new shapes\n",
        "print(f\"New shape of textData['vectorized_comments']: {textData['vectorized_comments'].shape}\")\n",
        "print(f\"New shape of dummy_y: {dummy_y.shape}\")\n",
        "\n",
        "# Now perform the train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    textData[\"vectorized_comments\"].tolist(),\n",
        "    dummy_y,\n",
        "    test_size=0.1,\n",
        "    random_state=56\n",
        ")\n"
      ],
      "metadata": {
        "id": "LRevb9FK38QG",
        "outputId": "80c2e1d6-5db6-4cc1-fb98-b9596c747bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of textData['vectorized_comments']: (1600,)\n",
            "Shape of dummy_y: (1600, 2)\n",
            "New shape of textData['vectorized_comments']: (1600,)\n",
            "New shape of dummy_y: (1600, 2)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b2a454a7-34dd-4eb5-93ac-ac891eb7e9fe",
        "_uuid": "4ebed90a6e28ab4de06c3037761106b6f6a0f4e5",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "id": "L9eTrYBgx50y"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    textData[\"vectorized_comments\"].T.tolist(),\n",
        "    dummy_y,\n",
        "    test_size=0.1,\n",
        "    random_state=56\n",
        ")\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "19c5322f-c579-4b9b-91fb-ebca06c6715f",
        "_uuid": "2c4e424e1c6f1a3a6abb7beb8eec6f33d3bc2622",
        "collapsed": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skmsroWsx50y",
        "outputId": "ae8c6a20-8a93-42ae-e1a3-bc5618f7f22a"
      },
      "cell_type": "code",
      "source": [
        "# Assuming textData[\"vectorized_comments\"] has shape (1600, 512)\n",
        "X = np.array(textData[\"vectorized_comments\"].tolist()).reshape((1600, 512))\n",
        "y = np.array(dummy_y).reshape((1600, 2))\n",
        "\n",
        "# Reshape train and test sets accordingly\n",
        "X_train2 = np.array(X_train).reshape((1440, 512))\n",
        "y_train2 = np.array(y_train).reshape((1440, 2))\n",
        "X_test2 = np.array(X_test).reshape((160, 512))\n",
        "y_test2 = np.array(y_test).reshape((160, 2))\n",
        "\n",
        "# Verify the shapes\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "print(f\"Shape of X_train2: {X_train2.shape}\")\n",
        "print(f\"Shape of y_train2: {y_train2.shape}\")\n",
        "print(f\"Shape of X_test2: {X_test2.shape}\")\n",
        "print(f\"Shape of y_test2: {y_test2.shape}\")\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (1600, 512)\n",
            "Shape of y: (1600, 2)\n",
            "Shape of X_train2: (1440, 512)\n",
            "Shape of y_train2: (1440, 2)\n",
            "Shape of X_test2: (160, 512)\n",
            "Shape of y_test2: (160, 2)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "00a664f3-390b-4f40-9580-8ca6c3021678",
        "_uuid": "87b1e9251955272eee9475662179a60dc030a399",
        "collapsed": true,
        "trusted": true,
        "id": "SiPUaUDex50z"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "Xtemp = textData[\"vectorized_comments\"].T.tolist()\n",
        "ytemp = data['given_class']\n",
        "training_indices = []\n",
        "testing_indices = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "skf.get_n_splits(Xtemp, ytemp)\n",
        "\n",
        "for train_index, test_index in skf.split(Xtemp, ytemp):\n",
        "    training_indices.append(train_index)\n",
        "    testing_indices.append(test_index)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0fac1165-69ad-4576-9729-8857d625cef1",
        "_uuid": "45bc95366bbb2e542be56aaf5f73582fd65062d4",
        "collapsed": true,
        "trusted": true,
        "id": "E0cDH7ECx500"
      },
      "cell_type": "code",
      "source": [
        "def extractTrainingAndTestingData(givenIndex):\n",
        "    X_train3 = np.zeros(shape=(1440, max(maxlength)+10, 512)).astype(np.float32)\n",
        "    Y_train3 = np.zeros(shape=(1440, 4)).astype(np.float32)\n",
        "    X_test3 = np.zeros(shape=(160, max(maxlength)+10, 512)).astype(np.float32)\n",
        "    Y_test3 = np.zeros(shape=(160, 4)).astype(np.float32)\n",
        "\n",
        "    empty_word = np.zeros(512).astype(np.float32)\n",
        "\n",
        "    count_i = 0\n",
        "    for i in training_indices[givenIndex]:\n",
        "        len1 = len(sen[i][0])\n",
        "        average_vector1 = np.zeros(512).astype(np.float32)\n",
        "        average_vector2 = np.zeros(512).astype(np.float32)\n",
        "        average_vector3 = np.zeros(512).astype(np.float32)\n",
        "        for j in range(max(maxlength)+10):\n",
        "            if j < len1:\n",
        "                X_train3[count_i,j,:] = doc2vec_model[sen[i][0][j]]\n",
        "                average_vector1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            #elif j >= len1 and j < len1 + 379:\n",
        "            #    X_train3[count_i,j,:] = glove_data[i, j-len1, :]\n",
        "            elif j == len1:\n",
        "                X_train3[count_i,j,:] = tfidf_data1[i]\n",
        "            elif j == len1 + 1:\n",
        "                X_train3[count_i,j,:] = tfidf_data2[i]\n",
        "            elif j == len1+2:\n",
        "                X_train3[count_i,j,:] = tfidf_data3[i]\n",
        "            elif j == len1+3:\n",
        "                X_train3[count_i,j,:] = average_vector1\n",
        "            elif j == len1+4:\n",
        "                X_train3[count_i,j,:] = average_vector2\n",
        "            elif j == len1+5:\n",
        "                X_train3[count_i,j,:] = average_vector3\n",
        "            elif j == len1+6:\n",
        "                X_train3[count_i,j,:] = final_pos_tags_data[i]\n",
        "            elif j == len1+7:\n",
        "                X_train3[count_i,j,:] = final_pos_data[i]\n",
        "            elif j == len1+8:\n",
        "                X_train3[count_i,j,:] = final_tokens_data[i]\n",
        "            elif j == len1+9:\n",
        "                X_train3[count_i,j,:] = final_dep_data[i]\n",
        "            else:\n",
        "                X_train3[count_i,j,:] = empty_word\n",
        "\n",
        "        Y_train3[count_i,:] = dummy_y[i]\n",
        "        count_i += 1\n",
        "\n",
        "\n",
        "    count_i = 0\n",
        "    for i in testing_indices[givenIndex]:\n",
        "        len1 = len(sen[i][0])\n",
        "        average_vector1 = np.zeros(512).astype(np.float32)\n",
        "        average_vector2 = np.zeros(512).astype(np.float32)\n",
        "        average_vector3 = np.zeros(512).astype(np.float32)\n",
        "        for j in range(max(maxlength)+10):\n",
        "            if j < len1:\n",
        "                X_test3[count_i,j,:] = doc2vec_model[sen[i][0][j]]\n",
        "                average_vector1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            #elif j >= len1 and j < len1 + 379:\n",
        "            #    X_test3[count_i,j,:] = glove_data[i, j-len1, :]\n",
        "            elif j == len1:\n",
        "                X_test3[count_i,j,:] = tfidf_data1[i]\n",
        "            elif j == len1 + 1:\n",
        "                X_test3[count_i,j,:] = tfidf_data2[i]\n",
        "            elif j == len1+2:\n",
        "                X_test3[count_i,j,:] = tfidf_data3[i]\n",
        "            elif j == len1+3:\n",
        "                X_test3[count_i,j,:] = average_vector1\n",
        "            elif j == len1+4:\n",
        "                X_test3[count_i,j,:] = average_vector2\n",
        "            elif j == len1+5:\n",
        "                X_test3[count_i,j,:] = average_vector3\n",
        "            elif j == len1+6:\n",
        "                X_test3[count_i,j,:] = final_pos_tags_data[i]\n",
        "            elif j == len1+7:\n",
        "                X_test3[count_i,j,:] = final_pos_data[i]\n",
        "            elif j == len1+8:\n",
        "                X_test3[count_i,j,:] = final_tokens_data[i]\n",
        "            elif j == len1+9:\n",
        "                X_test3[count_i,j,:] = final_dep_data[i]\n",
        "            else:\n",
        "                X_test3[count_i,j,:] = empty_word\n",
        "\n",
        "        Y_test3[count_i,:] = dummy_y[i]\n",
        "        count_i += 1\n",
        "\n",
        "    return X_train3, X_test3, Y_train3, Y_test3\n",
        ""
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1c0d8684-466d-48e6-9afa-bc30ff388a26",
        "_uuid": "b7d99866c070dbf1851fa20d0a242cf1623997ee",
        "collapsed": true,
        "scrolled": true,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "L-Mvh9TSx500",
        "outputId": "1f253a43-60fc-473d-af99-4c3d452120a1"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=128, kernel_size=9, padding='same', activation='relu', input_shape=(max(maxlength)+10,512)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters=128, kernel_size=7, padding='same', activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "#model.add(Dropout(0.25))\n",
        "\n",
        "#model.add(Bidirectional(LSTM(50, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(50, dropout=0.25, recurrent_dropout=0.2)))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m380\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m589,952\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m380\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m190\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m190\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m190\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m114,816\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m190\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m82,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m95\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m71,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m404\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">380</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">380</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">190</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">190</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">190</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">114,816</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">190</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">95</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">71,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">404</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m858,820\u001b[0m (3.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">858,820</span> (3.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m858,820\u001b[0m (3.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">858,820</span> (3.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming dummy_y should have 4 columns, you need to adjust its creation\n",
        "# For demonstration, let's assume you need to concatenate dummy_y with itself to create 4 columns\n",
        "dummy_y = np.concatenate([dummy_y, dummy_y], axis=1)\n",
        "\n",
        "# Verify the new shape of dummy_y\n",
        "print(f\"New shape of dummy_y: {dummy_y.shape}\")\n",
        "\n",
        "# Now you can use the extractTrainingAndTestingData function\n",
        "X_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(9)\n"
      ],
      "metadata": {
        "id": "dIwEvhzr6dBO",
        "outputId": "1cc89dd8-999c-46ab-bce9-10c927329f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New shape of dummy_y: (1600, 4)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "a8ec219d-90d2-4f03-97ac-52323bd112c8",
        "_uuid": "a81ad9005cf0c09a134930e20e9e14d3809e3c82",
        "collapsed": true,
        "scrolled": false,
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8B2yOi2x501",
        "outputId": "e381ab07-3978-4346-96bf-de5f0f27b700"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def train_and_evaluate_model(index, model, extract_data_func):\n",
        "    filename = f'weights.best.from_scratch{index}.keras'\n",
        "    checkpointer = ModelCheckpoint(filepath=filename, verbose=1, save_best_only=True)\n",
        "    X_train, X_test, Y_train, Y_test = extract_data_func(index)\n",
        "    model.fit(X_train, Y_train, epochs=10, batch_size=512, callbacks=[checkpointer], validation_data=(X_test, Y_test))\n",
        "    model.load_weights(filename)\n",
        "    predicted = np.rint(model.predict(X_test))\n",
        "    accuracy = accuracy_score(Y_test, predicted)\n",
        "    return accuracy\n",
        "\n",
        "final_accuracies = []\n",
        "\n",
        "# Initial training and evaluation\n",
        "initial_index = 9\n",
        "accuracy = train_and_evaluate_model(initial_index, model, extractTrainingAndTestingData)\n",
        "final_accuracies.append(accuracy)\n",
        "print(f'Accuracy for index {initial_index}: {accuracy}')\n",
        "\n",
        "# Loop through other indices\n",
        "for i in range(10):\n",
        "    accuracy = train_and_evaluate_model(i, model, extractTrainingAndTestingData)\n",
        "    final_accuracies.append(accuracy)\n",
        "    print(f'Accuracy for index {i}: {accuracy}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.4133 - loss: 0.6685 \n",
            "Epoch 1: val_loss improved from inf to 0.56200, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 14s/step - accuracy: 0.4075 - loss: 0.6599 - val_accuracy: 0.5312 - val_loss: 0.5620\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3488 - loss: 0.5753 \n",
            "Epoch 2: val_loss did not improve from 0.56200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 14s/step - accuracy: 0.3496 - loss: 0.5736 - val_accuracy: 0.7500 - val_loss: 0.5653\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3956 - loss: 0.5720 \n",
            "Epoch 3: val_loss did not improve from 0.56200\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14s/step - accuracy: 0.3957 - loss: 0.5709 - val_accuracy: 0.7500 - val_loss: 0.5643\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3850 - loss: 0.5601 \n",
            "Epoch 4: val_loss improved from 0.56200 to 0.56068, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 14s/step - accuracy: 0.3832 - loss: 0.5616 - val_accuracy: 0.7500 - val_loss: 0.5607\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3570 - loss: 0.5621 \n",
            "Epoch 5: val_loss improved from 0.56068 to 0.55733, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 14s/step - accuracy: 0.3544 - loss: 0.5623 - val_accuracy: 0.7500 - val_loss: 0.5573\n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3235 - loss: 0.5756 \n",
            "Epoch 6: val_loss improved from 0.55733 to 0.55589, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 14s/step - accuracy: 0.3244 - loss: 0.5725 - val_accuracy: 0.7500 - val_loss: 0.5559\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3671 - loss: 0.5478 \n",
            "Epoch 7: val_loss improved from 0.55589 to 0.53766, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14s/step - accuracy: 0.3705 - loss: 0.5509 - val_accuracy: 0.7500 - val_loss: 0.5377\n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3654 - loss: 0.5519 \n",
            "Epoch 8: val_loss improved from 0.53766 to 0.53402, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14s/step - accuracy: 0.3633 - loss: 0.5519 - val_accuracy: 0.7437 - val_loss: 0.5340\n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14s/step - accuracy: 0.3661 - loss: 0.5377 \n",
            "Epoch 9: val_loss improved from 0.53402 to 0.50940, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 15s/step - accuracy: 0.3649 - loss: 0.5379 - val_accuracy: 0.7250 - val_loss: 0.5094\n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3582 - loss: 0.5275 \n",
            "Epoch 10: val_loss improved from 0.50940 to 0.48988, saving model to weights.best.from_scratch9.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 14s/step - accuracy: 0.3567 - loss: 0.5246 - val_accuracy: 0.6938 - val_loss: 0.4899\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 290ms/step\n",
            "Accuracy for index 9: 0.0\n",
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.3339 - loss: 0.5024 \n",
            "Epoch 1: val_loss improved from inf to 0.45558, saving model to weights.best.from_scratch0.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 13s/step - accuracy: 0.3317 - loss: 0.5007 - val_accuracy: 0.4437 - val_loss: 0.4556\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3477 - loss: 0.4752 \n",
            "Epoch 2: val_loss improved from 0.45558 to 0.43924, saving model to weights.best.from_scratch0.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 14s/step - accuracy: 0.3460 - loss: 0.4746 - val_accuracy: 0.3125 - val_loss: 0.4392\n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - accuracy: 0.3625 - loss: 0.4582 \n",
            "Epoch 3: val_loss improved from 0.43924 to 0.42945, saving model to weights.best.from_scratch0.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14s/step - accuracy: 0.3592 - loss: 0.4566 - val_accuracy: 0.2500 - val_loss: 0.4295\n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.3113 - loss: 0.4399 \n",
            "Epoch 4: val_loss improved from 0.42945 to 0.42281, saving model to weights.best.from_scratch0.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 14s/step - accuracy: 0.3126 - loss: 0.4384 - val_accuracy: 0.2125 - val_loss: 0.4228\n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.2969 - loss: 0.3985 \n",
            "Epoch 5: val_loss improved from 0.42281 to 0.39958, saving model to weights.best.from_scratch0.keras\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 14s/step - accuracy: 0.2923 - loss: 0.4022 - val_accuracy: 0.2062 - val_loss: 0.3996\n",
            "Epoch 6/10\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "d7883c70-f74b-4389-85fe-ff575dd15a61",
        "_uuid": "2d96181354f0499ae115b2041997108afa8946dd",
        "collapsed": true,
        "trusted": true,
        "id": "qRtLyU1Rx502"
      },
      "cell_type": "code",
      "source": [
        "print(sum(final_accuracies) / len(final_accuracies))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a81ce0af-44a8-4081-8a54-73c8ffc97bdf",
        "_uuid": "e9c5ad94f4a4c5ad58e9e588782fc544839b8169",
        "collapsed": true,
        "trusted": true,
        "id": "t7CMc2yux503"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}